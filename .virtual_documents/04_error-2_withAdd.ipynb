


from __future__ import print_function

%matplotlib inline
import numpy
import matplotlib.pyplot as plt



































f = numpy.exp(1.)
F = 2.71
print('f = {}'.format(f))
print('F = {}'.format(F))


e = numpy.abs(f - F)
r = e/numpy.abs(f)
print('Absolute Error: {}'.format(e))
print('Relative Error: {}'.format(r))


p = int(-numpy.log10(r/5.))
print('Decimal precision: {}'.format(p))



































dx = numpy.linspace(1.0, 1e-4, 100)

fig = plt.figure()
fig.set_figwidth(fig.get_figwidth() * 2.0)
axes = []
axes.append(fig.add_subplot(1, 2, 1))
axes.append(fig.add_subplot(1, 2, 2))

for n in range(1, 5):
    axes[0].plot(dx, dx**n, label="$\Delta x^%s$" % n)
    axes[1].loglog(dx, dx**n, label="$\Delta x^%s$" % n)

axes[0].legend(loc=2)
axes[1].set_xticks([10.0**(-n) for n in range(5)])
axes[1].set_yticks([10.0**(-n) for n in range(16)])
axes[1].legend(loc=4)
for n in range(2):
    axes[n].set_title("Growth of Error vs. $\Delta x^n$")
    axes[n].set_xlabel("$\Delta x$")
    axes[n].set_ylabel("Estimated Error")

plt.show()





























import sympy
sympy.init_printing(pretty_print=True)
x = sympy.symbols('x')
f = sympy.exp(x)
f.series(x0=0, n=3)





x = numpy.linspace(-1, 1, 100)
f = numpy.exp(x)
T_N = 1.0 + x + x**2 / 2.0
R_N = numpy.exp(1) * x**3 / 6.0


fig = plt.figure(figsize=(8,6))
axes = fig.add_subplot(1,1,1)
axes.plot(x, T_N, 'r', x, f, 'k', x, numpy.abs(R_N), 'b')
axes.plot(x,numpy.abs(numpy.exp(x)-T_N),'g--')
axes.plot(0.0, 1.0, 'o', markersize=10)

axes.grid()
axes.set_xlabel("x",fontsize=16)
axes.set_ylabel("$f(x)$, $T_N(x)$, $|R_N(x)|$", fontsize=16)
axes.legend(["$T_N(x)$", "$f(x)$", "$|R_N(x)|$", "e(x)"], loc=2)
plt.show()











x = numpy.linspace(0.8, 2, 100)
f = 1.0 / x
T_N = 1.0 - (x-1) + (x-1)**2
R_N = -(x-1.0)**3 / (1.**4)


plt.figure(figsize=(8,6))
plt.plot(x, T_N, 'r', x, f, 'k', x, numpy.abs(R_N), 'b')
plt.plot(x,numpy.abs(f - T_N),'g--')
plt.plot(1.0, 1.0, 'o', markersize=10)
plt.grid(True)
plt.xlabel("x",fontsize=16)
plt.ylabel("$f(x)$, $T_N(x)$, $R_N(x)$",fontsize=16)
plt.title('$f(x) = 1/x$',fontsize=18)
plt.legend(["$T_N(x)$", "$f(x)$", "$|R_N(x)|$", '$e(x)$'], loc='best')
plt.show()


























def eval_poly(p, x):
    '''Evaluates a polynomial using Horner's method given coefficients p at x
    
      The polynomial is defined as
    
        P(x) = p[0] x**n + p[1] x**(n-1) + ... + p[n-1] x + p[n]
        
    Parameters:
        p: list or numpy array of coefficients 
        x:  scalar float or numpy array (this version is more robust to floating point error)
        
    returns:
        P(x):  value of the polynomial at point x, P will return as the same type as x
    
    '''
    
    
    if isinstance(x,numpy.ndarray):
        y = p[0]*numpy.ones(x.shape)
    elif isinstance(x,float):
        y = p[0]
    else:
        raise TypeError
        
    for element in p[1:]:
        y = y*x + element
        
    return y 


# Scalar test

p = [1, 2, 3]
x = 1.
test = eval_poly(p,x)
answer = numpy.array([x**2, x, 1]).dot(p)

print('test = {} ({}), answer = {} ({})'.format(test,type(test),answer,type(answer)))
numpy.testing.assert_allclose(test,answer)
print('success')


# Vectorized test with x a numpy array

p = [1, -3, 10, 4, 5, 5]
x = numpy.linspace(-10, 10, 100)
P = eval_poly(p,x)
print('x: {}, P(x): {}'.format(type(x),type(P)))


plt.plot(x, P)
plt.xlabel('x')
plt.ylabel('P(x)')
plt.title('{}th order polynomial, p={}'.format(len(p)-1,p))
plt.grid()
plt.show()





# Choose the number of iterations
N = 40
y = numpy.empty(N+1)            # Allocate an empty vector with N+1 entries

# Now use the difference equation to generate the numbers in the sequence
y[0] = 1
y[1] = 1/5
for n in range(2,N+1):
    y[n] = 16/5*y[n-1] - 3/5*y[n-2]






# Now plot the result
n = numpy.arange(0,N+1)
fig = plt.figure(figsize=(10.0, 5.0))
axes = fig.add_subplot(1, 1, 1)
axes.semilogy(n,y, 'rx', markersize=5, label='$y_n$')
axes.semilogy(n,(1/5)**n,'b.', label='$y_{true}$')
axes.grid()
axes.set_title("Calculated Values Of A Difference Equation",fontsize=18)
axes.set_xlabel("$n$",fontsize=16)
axes.set_ylabel("$y_n$",fontsize=16)
axes.legend(loc='best', shadow=True)
plt.show()
























































d_1_values = [1, 2, 3, 4, 5, 6, 7, 8, 9]
d_2_values = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
E_values = [0, -1, -2,]

fig = plt.figure(figsize=(10.0,1.5))
axes = fig.add_subplot(1, 1, 1)

for E in E_values:
    for d1 in d_1_values:
        for d2 in d_2_values:
            axes.plot( (d1 + d2 * 0.1) * 10**E, 0.0, 'r|', markersize=20)
            axes.plot(-(d1 + d2 * 0.1) * 10**E, 0.0, 'r|', markersize=20)
            
axes.plot(0.0, 0.0, '|', markersize=20)
axes.plot([-1., 1.], [0.0, 0.0], 'k|', markersize=30)

axes.plot([-10.0, 10.0], [0.0, 0.0], 'k')

axes.set_title("Distribution of Values $[-10, 10]$")
axes.set_yticks([])
ticks = [i for i in range(-10,11,1)]
axes.set_xticks(ticks)
axes.set_xlabel("x")
axes.set_ylabel("")
axes.set_xlim([-10, 10])
plt.show()


fig = plt.figure(figsize=(10.0,1.5))
axes = fig.add_subplot(1,1,1)

for E in E_values:
    for d1 in d_1_values:
        for d2 in d_2_values:
            axes.plot( (d1 + d2 * 0.1) * 10**E, 0.0, 'r+', markersize=20)
            axes.plot(-(d1 + d2 * 0.1) * 10**E, 0.0, 'r+', markersize=20)
            
axes.plot(0.0, 0.0, '+', markersize=20)
axes.plot([-0.1, 0.1], [0.0, 0.0], 'k|', markersize=30)
axes.plot([-1, 1], [0.0, 0.0], 'k')

axes.set_title("Close up $[-1, 1]$")
axes.set_yticks([])
ticks = numpy.linspace(-1.,1.,21)
axes.set_xticks(ticks)
axes.set_xlabel("x")
axes.set_ylabel("")
axes.set_xlim([-1, 1])
#fig.tight_layout(h_pad=1, w_pad=5)

plt.show()


























d_1_values = [1]
d_2_values = [0, 1]
E_values = [1, 0, -1]

fig = plt.figure(figsize=(10.0, 1.0))
axes = fig.add_subplot(1, 1, 1)

for E in E_values:
    for d1 in d_1_values:
        for d2 in d_2_values:
            axes.plot( (d1 + d2 * 0.5) * 2**E, 0.0, 'r+', markersize=20)
            axes.plot(-(d1 + d2 * 0.5) * 2**E, 0.0, 'r+', markersize=20)
            
axes.plot(0.0, 0.0, 'r+', markersize=20)
axes.plot([-4.5, 4.5], [0.0, 0.0], 'k')

axes.set_title("Distribution of Values")
axes.set_yticks([])
axes.set_xticks(numpy.linspace(-4,4,9))
axes.set_xlabel("x")
axes.set_ylabel("")
axes.grid()
axes.set_xlim([-5, 5])
plt.show()

















print(numpy.finfo(numpy.float16))


print(numpy.finfo(numpy.float32))


print(numpy.finfo(float))


print(numpy.finfo(numpy.float128))





eps = numpy.finfo(float).eps
MAX = numpy.finfo(float).max
print('eps = {}'.format(eps))
print('MAX = {}'.format(MAX))





print(MAX)


print(MAX*(1+ 0.4*eps))


print(1 + 0.4*eps == 1.0)








eps = numpy.finfo(float).eps
delta = 0.5*eps
x = 1+delta  -1
y = 1 - 1 + delta
print('1 + delta - 1 = {}'.format(x))
print('1 - 1 + delta = {}'.format(y))
print( x == y)














dx = numpy.array([10**(-n) for n in range(1, 16)])
x = 1.0 + dx
y = numpy.ones(x.shape)
error = numpy.abs(x - y - dx) / (dx)


fig = plt.figure()
fig.set_figwidth(fig.get_figwidth() * 2)

axes = fig.add_subplot(1, 2, 1)
axes.loglog(dx, x + y, 'o-')
axes.set_xlabel("$\Delta x$")
axes.set_ylabel("$x + y$")
axes.set_title("$\Delta x$ vs. $x+y$")

axes = fig.add_subplot(1, 2, 2)
axes.loglog(dx, error, 'o-')
axes.set_xlabel("$\Delta x$")
axes.set_ylabel("$|x + y - \Delta x| / \Delta x$")
axes.set_title("Difference between $x$ and $y$ vs. Relative Error")

plt.show()








x = numpy.linspace(-1e-3, 1e-3, 100, dtype=numpy.float32)
f = 0.5
F = (1.0 - numpy.cos(x)) / x**2
rel_err = numpy.abs((f - F)) / f


fig = plt.figure(figsize=(8,6))
axes = fig.add_subplot(1, 1, 1)
axes.plot(x, rel_err, 'o')
axes.set_xlabel("x")
axes.grid()
axes.set_ylabel("Relative Error")
axes.set_title("$\\frac{1-\\cos{x}}{x^2} - \\frac{1}{2}$",fontsize=18)
plt.show()





x = numpy.linspace(0.988, 1.012, 1000, dtype=numpy.float16)
y = x**7 - 7.0 * x**6 + 21.0 * x**5 - 35.0 * x**4 + 35.0 * x**3 - 21.0 * x**2 + 7.0 * x - 1.0

# repeat using Horner's method from above
p = numpy.array([1, -7, 21, -35, 35, -21, 7, -1 ])
yh = eval_poly(p,x)


fig = plt.figure(figsize=(8,6))
fig.set_figwidth(fig.get_figwidth() * 2)

axes = fig.add_subplot(1, 2, 1)
axes.plot(x, y, 'r',label='naive')
axes.plot(x, yh, 'b',label='horner')
axes.set_xlabel("x")
axes.set_ylabel("y")
axes.set_ylim((-0.1, 0.1))
axes.set_xlim((x[0], x[-1]))
axes.grid()
axes.legend()

axes = fig.add_subplot(1, 2, 2)
axes.plot(x,yh-y,'g')
axes.grid()
axes.set_xlabel('x')
axes.set_ylabel('$f_{horner} - f_n$')
axes.set_title('error')
plt.show()


def eval_polys(p, x):
    '''Evaluates a polynomial using Horner's method given coefficients p at x
    
      The polynomial is defined as
    
        P(x) = p[0] x**n + p[1] x**(n-1) + ... + p[n-1] x + p[n]
        
    Parameters:
        p: list or numpy array of coefficients 
        x:  scalar float or numpy array this version is less careful about input type
        
    returns:
        P(x):  value of the polynomial at point x, P will return as the same type as x
    
    '''
   
    y = p[0]
    for element in p[1:]:
        y = y*x + element
        
    return y 



# repeat using different Horner's method from above
yh = eval_polys(p,x)


fig = plt.figure(figsize=(8,6))
fig.set_figwidth(fig.get_figwidth() * 2)

axes = fig.add_subplot(1, 2, 1)
axes.plot(x, y, 'r',label='naive')
axes.plot(x, yh, 'b',label='horner')
axes.set_xlabel("x")
axes.set_ylabel("y")
axes.set_ylim((-0.1, 0.1))
axes.set_xlim((x[0], x[-1]))
axes.grid()
axes.legend()

axes = fig.add_subplot(1, 2, 2)
axes.plot(x,yh-y,'g')
axes.grid()
axes.set_xlabel('x')
axes.set_ylabel('$f_{horner} - f_n$')
axes.set_title('error')
plt.show()





x = numpy.linspace(0.5, 1.5, 101, dtype=numpy.float32)
f_hat = (x**2 - 1.0) / (x - 1.0)
f = (x + 1.0)


fig = plt.figure()
axes = fig.add_subplot(1, 1, 1)
axes.plot(x, numpy.abs(f - f_hat)/numpy.abs(f))
axes.set_xlabel("$x$")
axes.set_ylabel("Relative Error")
axes.grid()
plt.show()














f = lambda x: numpy.exp(x)
f_prime = lambda x: numpy.exp(x)


delta_x = numpy.array([2.0**(-n) for n in range(1, 60)])
x = 1.0

# Forward finite difference approximation to first derivative
f_hat_1 = (f(x + delta_x) - f(x)) / (delta_x)
# Centered finite difference approximation to first derivative
f_hat_2 = (f(x + delta_x) - f(x - delta_x)) / (2.0 * delta_x)


fig = plt.figure(figsize=(8,6))
axes = fig.add_subplot(1, 1, 1)
axes.loglog(delta_x, numpy.abs(f_hat_1 - f_prime(x)), 'o-', label="One-Sided")
axes.loglog(delta_x, numpy.abs(f_hat_2 - f_prime(x)), 's-', label="Centered")
axes.legend(loc=3,fontsize=14)
axes.set_xlabel("$\Delta x$",fontsize=16)
axes.set_ylabel("Absolute Error",fontsize=16)
axes.set_title("Finite Difference approximations to $df/dx$",fontsize=18)
axes.grid()
plt.show()





from scipy.special import factorial

def my_exp(x, N=10):
    value = 0.0
    for n in range(N + 1):
        value += x**n / float(factorial(n))
        
    return value





eps = numpy.finfo(numpy.float64).eps

x = numpy.linspace(-2, 50., 100, dtype=numpy.float64)
MAX_N = 300
for N in range(1, MAX_N + 1):
    rel_error = numpy.abs((numpy.exp(x) - my_exp(x, N=N)) / numpy.exp(x))
    if numpy.all(rel_error < 8.0 * eps): 
        break


fig = plt.figure(figsize=(8,6))
axes = fig.add_subplot(1, 1, 1)
axes.plot(x, rel_error/eps)
axes.set_xlabel("x")
axes.set_ylabel("Relative Error/eps")
axes.set_title('N = {} terms'.format(N))
axes.grid()
plt.show()





print(numpy.log(numpy.finfo(float).max))





print(numpy.exp(709,dtype=numpy.float64))
print(numpy.exp(-709,dtype=numpy.float64))


























# Based on the code by Nick Higham
# https://gist.github.com/higham/6f2ce1cdde0aae83697bca8577d22a6e
# Compares relative error formulations using single precision and compared to double precision

N = 501    # Note: Use 501 instead of 500 to avoid the zero value
d = numpy.finfo(numpy.float32).eps * 1e4
a = 3.0
x = a * numpy.ones(N, dtype=numpy.float32)
y = [x[i] + numpy.multiply((i - numpy.divide(N, 2.0, dtype=numpy.float32)), d, dtype=numpy.float32) for i in range(N)]

# Compute errors and "true" error
relative_error = numpy.empty((2, N), dtype=numpy.float32)
relative_error[0, :] = numpy.abs(x - y) / x
relative_error[1, :] = numpy.abs(1.0 - y / x)
exact = numpy.abs( (numpy.float64(x) - numpy.float64(y)) / numpy.float64(x))

# Compute differences between error calculations
error = numpy.empty((2, N))
for i in range(2):
    error[i, :] = numpy.abs((relative_error[i, :] - exact) / numpy.abs(exact))

fig = plt.figure(figsize=(8,6))
axes = fig.add_subplot(1, 1, 1)
axes.semilogy(y, error[0, :], '.', markersize=10, label="$|x-y|/|x|$")
axes.semilogy(y, error[1, :], '.', markersize=10, label="$|1-y/x|$")

axes.grid(True)
axes.set_xlabel("y")
axes.set_ylabel("Relative Error")
axes.set_xlim((numpy.min(y), numpy.max(y)))
axes.set_ylim((5e-9, numpy.max(error[1, :])))
axes.set_title("Relative Error Comparison: x,y {}".format(y[0].dtype))
axes.legend()
plt.show()
















































